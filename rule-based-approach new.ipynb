{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bace2e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stanza in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: protobuf in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from stanza) (4.21.1)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from stanza) (1.12.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from stanza) (1.20.3)\n",
      "Requirement already satisfied: six in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from stanza) (1.16.0)\n",
      "Requirement already satisfied: emoji in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from stanza) (2.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from stanza) (4.62.3)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from stanza) (2.26.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.3.0->stanza) (3.10.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->stanza) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->stanza) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->stanza) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->stanza) (1.26.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tqdm->stanza) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0afd6e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e5d4ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f69587906e34810b3d1e6f5c921eb2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 15:03:45 INFO: \"hindi\" is an alias for \"hi\"\n",
      "2023-02-13 15:03:45 INFO: Downloading default packages for language: hi (Hindi) ...\n",
      "2023-02-13 15:03:46 INFO: File exists: C:\\Users\\lenovo\\stanza_resources\\hi\\default.zip\n",
      "2023-02-13 15:03:51 INFO: Finished downloading models and saved to C:\\Users\\lenovo\\stanza_resources.\n",
      "2023-02-13 15:03:51 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff897ea4bdd54d46925f83c20cdf43ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 15:03:52 INFO: Loading these models for language: hi (Hindi):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | hdtb    |\n",
      "| pos       | hdtb    |\n",
      "| lemma     | hdtb    |\n",
      "| depparse  | hdtb    |\n",
      "=======================\n",
      "\n",
      "2023-02-13 15:03:52 INFO: Use device: cpu\n",
      "2023-02-13 15:03:52 INFO: Loading: tokenize\n",
      "2023-02-13 15:03:52 INFO: Loading: pos\n",
      "2023-02-13 15:03:53 INFO: Loading: lemma\n",
      "2023-02-13 15:03:53 INFO: Loading: depparse\n",
      "2023-02-13 15:03:54 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza.download('hindi')\n",
    "nlp = stanza.Pipeline('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7d83f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 15:03:54 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4f30f089574306881f8cbde6817e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 15:03:55 INFO: Loading these models for language: hi (Hindi):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | hdtb    |\n",
      "| pos       | hdtb    |\n",
      "| lemma     | hdtb    |\n",
      "=======================\n",
      "\n",
      "2023-02-13 15:03:55 INFO: Use device: cpu\n",
      "2023-02-13 15:03:55 INFO: Loading: tokenize\n",
      "2023-02-13 15:03:55 INFO: Loading: pos\n",
      "2023-02-13 15:03:56 INFO: Loading: lemma\n",
      "2023-02-13 15:03:56 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang =\"hi\", processors = \"tokenize,pos,lemma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6768d8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary that contains pos tags and their explanations\n",
    "pos_dict = {\n",
    "'CC': 'coordinating conjunction','CD': 'cardinal digit','DT': 'determiner',\n",
    "'EX': 'existential there (like: \\\"there is\\\" ... think of it like \\\"there exists\\\")',\n",
    "'FW': 'foreign word','IN':  'preposition/subordinating conjunction','JJ': 'adjective \\'big\\'',\n",
    "'JJR': 'adjective, comparative \\'bigger\\'','JJS': 'adjective, superlative \\'biggest\\'',\n",
    "'LS': 'list marker 1)','MD': 'modal could, will','NN': 'noun, singular \\'desk\\'',\n",
    "'NNS': 'noun plural \\'desks\\'','NNP': 'proper noun, singular \\'Harrison\\'',\n",
    "'NNPS': 'proper noun, plural \\'Americans\\'','PDT': 'predeterminer \\'all the kids\\'',\n",
    "'POS': 'possessive ending parent\\'s','PRP': 'personal pronoun I, he, she',\n",
    "'PRP$': 'possessive pronoun my, his, hers','RB': 'adverb very, silently,',\n",
    "'RBR': 'adverb, comparative better','RBS': 'adverb, superlative best',\n",
    "'RP': 'particle give up','TO': 'to go \\'to\\' the store.','UH': 'interjection errrrrrrrm',\n",
    "'VB': 'verb, base form take','VBD': 'verb, past tense took',\n",
    "'VBG': 'verb, gerund/present participle taking','VBN': 'verb, past participle taken',\n",
    "'VBP': 'verb, sing. present, non-3d take','VBZ': 'verb, 3rd person sing. present takes',\n",
    "'WDT': 'wh-determiner which','WP': 'wh-pronoun who, what','WP$': 'possessive wh-pronoun whose',\n",
    "'WRB': 'wh-abverb where, when','QF' : 'quantifier, bahut, thoda, kam (Hindi)','VM' : 'main verb',\n",
    "'PSP' : 'postposition, common in indian langs','DEM' : 'demonstrative, common in indian langs'\n",
    "}\n",
    "\n",
    "#extract parts of speech\n",
    "def extract_pos(doc):\n",
    "    parsed_text = {'word':[], 'pos':[], 'exp':[]}\n",
    "    for sent in doc.sentences:\n",
    "        for wrd in sent.words:\n",
    "            if wrd.pos in pos_dict.keys():\n",
    "                pos_exp = pos_dict[wrd.pos]\n",
    "            else:\n",
    "                pos_exp = 'NA'\n",
    "            parsed_text['word'].append(wrd.text)\n",
    "            parsed_text['pos'].append(wrd.xpos)\n",
    "            parsed_text['exp'].append(wrd.upos)\n",
    "    #return a dataframe of pos and text\n",
    "    return pd.DataFrame(parsed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e14bd271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_approach(s):\n",
    "    hindi_doc = nlp(s)\n",
    "    df = extract_pos(hindi_doc)\n",
    "    dictionary = df[['word','exp']].set_index('word').to_dict()['exp']\n",
    "    dictionary['(']= \"NILL\"\n",
    "    dictionary[')']='NILL'\n",
    "    dictionary[\" \"]='NILL'\n",
    "    \n",
    "    original_array =  s.split()\n",
    "    \n",
    "    # CEP\n",
    "    ending =\"\"\n",
    "\n",
    "    i=0\n",
    "    while i< len(original_array):\n",
    "        ending+=original_array[i]\n",
    "        ending+=\" \"\n",
    "\n",
    "        if(dictionary[original_array[i]]=='VERB'):\n",
    "            j=i+1\n",
    "            while(j< len(original_array)):\n",
    "                if(dictionary[original_array[j]]=='AUX'):\n",
    "                    ending+=original_array[j]\n",
    "                    ending+=\" \"\n",
    "                    j=j+1\n",
    "                else:\n",
    "                    break\n",
    "            ending+=')'\n",
    "            ending+=\" \"\n",
    "            i=j-1\n",
    "        i=i+1\n",
    "\n",
    "\n",
    "    ending= ending.strip()\n",
    "    \n",
    "    modified_array = ending.split()\n",
    "    last = len(modified_array)\n",
    "\n",
    "    if modified_array[last-1]!=')':\n",
    "        ending+=\" \"\n",
    "        ending+=')'\n",
    "\n",
    "    modified_array = ending.split()\n",
    "    print(modified_array)\n",
    "    \n",
    "    # now mark CSP\n",
    "    # finite verb\n",
    "    starting =\"\"\n",
    "    \n",
    "    for i in range(len(modified_array)):\n",
    "        f=0\n",
    "        if(modified_array[i]!=')'):\n",
    "            if(dictionary[modified_array[i]]=='CCONJ'):\n",
    "                f=1\n",
    "            if(dictionary[modified_array[i]]=='SCONJ'):\n",
    "                f=2\n",
    "            if(dictionary[modified_array[i]]=='PRON'):\n",
    "                if(modified_array[i]=='जिसने'):\n",
    "                    f=2\n",
    "                if(modified_array[i]=='जिसे'):\n",
    "                    f=2\n",
    "                if(modified_array[i]=='जो'):\n",
    "                    f=2\n",
    "                if(modified_array[i]=='जिनको'):\n",
    "                    f=2\n",
    "\n",
    "\n",
    "        if f==1:\n",
    "            starting+=modified_array[i]\n",
    "            starting+=\" \"\n",
    "            starting+='('\n",
    "            starting+=\" \"\n",
    "\n",
    "        elif f==2:\n",
    "            starting+='('\n",
    "            starting+=\" \"\n",
    "            starting+=modified_array[i]\n",
    "            starting+=\" \"\n",
    "\n",
    "        elif f==0:\n",
    "            starting+=modified_array[i]\n",
    "            starting+=\" \"\n",
    "    \n",
    "    if starting[0]!='(':\n",
    "        starting= '('+\" \"+ starting\n",
    "        \n",
    "    starting = starting.strip()\n",
    "    print(starting)\n",
    "    \n",
    "    \n",
    "    # sanity checker\n",
    "    strt = starting\n",
    "    left=0\n",
    "    right=0\n",
    "\n",
    "    for i in range(len(strt)):\n",
    "        if(strt[i]=='('):\n",
    "            left=left+1\n",
    "        if(strt[i]==')'):\n",
    "            right=right+1\n",
    "\n",
    "#     print(left)\n",
    "#     print(right)\n",
    "\n",
    "    diff = abs(left-right)\n",
    "    if left>right:\n",
    "        while(diff):\n",
    "            strt+=')'\n",
    "            diff=diff-1\n",
    "\n",
    "    if right>left:\n",
    "        temp_strt = strt.split()\n",
    "        temp_st =\"\"\n",
    "        j=0\n",
    "        \n",
    "        while j<len(temp_strt):\n",
    "            if dictionary[temp_strt[j]]== 'PROPN':\n",
    "                if diff>0 & j-1>=0:\n",
    "                    if temp_strt[j-1]!='(':\n",
    "                        temp_st+='('\n",
    "                        temp_st+=\" \"\n",
    "                        diff=diff-1\n",
    "            temp_st+=temp_strt[j]\n",
    "            temp_st+=\" \"\n",
    "            j=j+1\n",
    "        \n",
    "        strt = temp_st    \n",
    "    \n",
    "    \n",
    "        while(diff):\n",
    "            strt='('+strt\n",
    "            diff=diff-1\n",
    "    \n",
    "    return strt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "367a9ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['राम', 'जो', 'खेल', 'रहा', 'था', ')', 'नहीं', 'आया', ')']\n",
      "( राम ( जो खेल रहा था ) नहीं आया )\n"
     ]
    }
   ],
   "source": [
    "final_string = rule_based_approach(\"राम जो खेल रहा था नहीं आया\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2703f0f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'( राम ( जो खेल रहा था ) नहीं आया )'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ff2204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
